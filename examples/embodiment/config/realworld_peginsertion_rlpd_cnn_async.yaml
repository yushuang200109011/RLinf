defaults:
  - env/realworld_peg_insertion@env.train
  - env/realworld_peg_insertion@env.eval
  - model/cnn_policy@actor.model
  - training_backend/fsdp@actor.fsdp_config
  - override hydra/job_logging: stdout

hydra:
  run:
    dir: .
  output_subdir: null
  searchpath:
    - file://${oc.env:EMBODIED_PATH}/config/

cluster:
  num_nodes: 2
  component_placement:
    actor: 
      node_group: "4090"
      placement: 0 # Local GPU rank in the 4090 node group (n for the n+1 th GPU)
    env:
      node_group: franka
      placement: 0 # Local robot rank in the franka node group (n for the n+1 th robot)
    rollout:
      node_group: "4090"
      placement: 0 # Local GPU rank in the 4090 node group (n for the n+1 th GPU)
  node_groups:
    - label: "4090" # GPU node group
      node_ranks: 0 # The ranks of the nodes to be included in this groupï¼Œ change this if your training node is not rank 0
    - label: franka
      node_ranks: 1 # Change this if your robot node is not rank 1
      hardware:
        type: Franka
        configs:
          - robot_ip: ROBOT_IP # Replace with your robot's IP address
            node_rank: 1

data:
  type: robot_demo
  channel:
    name: demo_data
  path: "/path/to/demo_data"

runner:
  task_type: embodied
  logger:
    log_path: "../results"
    project_name: rlinf
    experiment_name: "sac_mlp"
    logger_backends: ["tensorboard"] # wandb, swanlab

  max_epochs: 8000
  max_steps: -1

  only_eval: False
  val_check_interval: -1
  save_interval: -1

  resume_dir: null # Optional: path to a saved checkpoint directory, such as 'checkpoints/global_step_10'. If not None, it will be used to resume training.
  ckpt_path: null  # Optional: path to a .pt checkpoint. If not None, it will be loaded after the model is instantiated (for evaluation).

algorithm:
  update_epoch: 8
  group_size: 1
  agg_q: mean

  backup_entropy: False
  critic_subsample_size: 2
  eval_rollout_epoch: 1

  adv_type: embodied_sac
  loss_type: embodied_sac
  loss_agg_func: "token-mean"
  
  bootstrap_type: standard
  gamma: 0.96
  tau: 0.005  # Soft update coefficient for target networks
  auto_entropy_tuning: True  # Enable automatic entropy tuning
  alpha_type: softplus
  initial_alpha: 0.01  # Initial temperature value
  target_entropy: -3  # Target entropy (-action_dim)
  alpha_lr: 3.0e-4  # Learning rate for temperature parameter
  
  critic_actor_ratio: 4
  # Replay buffer settings
  replay_buffer_capacity: 20000
  min_buffer_size: 200  # Minimum buffer size before training starts
  target_update_freq: 1  # Frequency of target network updates

  # params for rollout
  sampling_params:
    do_sample: True
    temperature_train: 1.0
    temperature_eval: 0.6
    top_k: 50
    top_p: 1.0
    repetition_penalty: 1.0

  # length argument for autoregressive sampling
  length_params:
    max_new_token: 7
    max_length: 1024
    min_length: 1

env:
  group_name: "EnvGroup"
  enable_offload: False

  train:
    ignore_terminations: False
    total_num_envs: 1
    override_cfg:
      is_dummy: False
      use_dense_reward: False
      target_ee_pose: TARGET_EE_POSE # Replace with your target end-effector pose

rollout:
  group_name: "RolloutGroup"
  backend: "huggingface"
  enable_offload: False
  pipeline_stage_num: 1
  sync_weight_nccl_max_ctas: 32 # Max CTAs (Compute Thread Array, the compute resource of GPUs) for sync weight communication. Higher values consume more GPU resources, and thus sync weight is faster but could affect rollout speed in async mode. Positive integer 1 to 32. Default 32.

  model:
    model_path: "/path/to/model"
    precision: ${actor.model.precision}
    num_q_heads: 10

actor:
  group_name: "ActorGroup"
  training_backend: "fsdp"

  micro_batch_size: 256  # Smaller batch_size to accelerate training in real world
  global_batch_size: 256  # Smaller batch_size to accelerate training in real world
  seed: 1234
  enable_offload: False

  model:
    model_path: "/path/to/model"
    state_dim: 19
    action_dim: 6
    num_q_heads: 10 # number of Q heads for critic, passed to policy module

  optim:
    lr: 3.0e-4  # Lower learning rate for SAC
    value_lr: 3.0e-4  # Q-network learning rate
    adam_beta1: 0.9
    adam_beta2: 0.999
    adam_eps: 1.0e-08
    clip_grad: 10.0

  # Override the default values in training_backend/fsdp
  fsdp_config:
    strategy: "fsdp"
    sharding_strategy: "no_shard"
    mixed_precision:
      param_dtype: ${actor.model.precision}
      reduce_dtype: ${actor.model.precision}
      buffer_dtype: ${actor.model.precision}

reward:
  use_reward_model: False

critic:
  use_critic_model: False