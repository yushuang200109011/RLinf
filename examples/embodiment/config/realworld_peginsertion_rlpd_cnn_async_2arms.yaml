defaults:
  - env/realworld_peg_insertion@env.train
  - env/realworld_peg_insertion@env.eval
  - model/cnn_policy@actor.model
  - training_backend/fsdp@actor.fsdp_config
  - override hydra/job_logging: stdout

hydra:
  run:
    dir: .
  output_subdir: null
  searchpath:
    - file://${oc.env:EMBODIED_PATH}/config/

cluster:
  num_nodes: 3
  component_placement:
    actor: 
      node_group: "4090"
      placement: 0
    env:
      node_group: franka
      placement: 0-1
    rollout:
      node_group: "4090"
      placement: 0:0-1
  node_groups:
    - label: "4090"
      node_ranks: 0-0
    - label: franka
      node_ranks: 1-2
      hardware:
        type: Franka
        configs:
          - robot_ip: ROBOT_IP_FOR_RANK1
            node_rank: 1
          - robot_ip: ROBOT_IP_FOR_RANK2
            node_rank: 2

runner:
  task_type: embodied
  logger:
    log_path: "../results"
    project_name: rlinf
    experiment_name: "sac_mlp"
    logger_backends: ["tensorboard"] # wandb, swanlab

  max_epochs: 10
  max_steps: -1

  only_eval: False
  val_check_interval: -1
  save_interval: -1

  resume_dir: null # Optional: path to a saved checkpoint directory, such as 'checkpoints/global_step_10'. If not None, it will be used to resume training.
  ckpt_path: null  # Optional: path to a .pt checkpoint. If not None, it will be loaded after the model is instantiated (for evaluation).

algorithm:
  update_epoch: 32
  group_size: 1
  agg_q: min

  rollout_epoch: 1
  eval_rollout_epoch: 1

  adv_type: embodied_sac
  loss_type: embodied_sac
  loss_agg_func: "token-mean"
  
  bootstrap_type: always
  gamma: 0.8
  tau: 0.01  # Soft update coefficient for target networks
  auto_entropy_tuning: True  # Enable automatic entropy tuning
  alpha_type: softplus
  initial_alpha: 0.01  # Initial temperature value
  target_entropy: -4  # Target entropy (-action_dim)
  alpha_lr: 3.0e-4  # Learning rate for temperature parameter
  
  # Replay buffer settings
  replay_buffer_capacity: 10000
  min_buffer_size: 200  # Minimum buffer size before training starts
  target_update_freq: 1  # Frequency of target network updates

  # params for rollout
  sampling_params:
    do_sample: True
    temperature_train: 1.0
    temperature_eval: 0.6
    top_k: 50
    top_p: 1.0
    repetition_penalty: 1.0

  # length argument for autoregressive sampling
  length_params:
    max_new_token: 7
    max_length: 1024
    min_length: 1

env:
  group_name: "EnvGroup"
  enable_offload: False

  train:
    total_num_envs: 2
    override_cfg:
      target_ee_pose: [0.5, 0, 0.1, -3.14, 0, -1.57]
  
  eval:
    total_num_envs: 2
    override_cfg:
      is_dummy: True
      target_ee_pose: [0, 0, 0, 0, 0, 0]
      # Dummy placeholders for robot and camera
      robot_ip: 0.0.0.0
      camera_serials: ["0123456789", ]


rollout:
  group_name: "RolloutGroup"
  backend: "huggingface"
  enable_offload: False
  pipeline_stage_num: 1
  sync_weight_nccl_max_ctas: 32 # Max CTAs (Compute Thread Array, the compute resource of GPUs) for sync weight communication. Higher values consume more GPU resources, and thus sync weight is faster but could affect rollout speed in async mode. Positive integer 1 to 32. Default 32.

  model:
    model_path: "/path/to/model"
    precision: ${actor.model.precision}

actor:
  group_name: "ActorGroup"
  training_backend: "fsdp"

  micro_batch_size: 256  # Smaller batch_size to accelerate training in real world
  global_batch_size: 256  # Smaller batch_size to accelerate training in real world
  seed: 1234
  enable_offload: False

  model:
    model_path: "/path/to/model"
    state_dim: 19
    action_dim: 6
    num_q_heads: 10 # number of Q heads for critic, passed to policy module

  optim:
    lr: 3.0e-4  # Lower learning rate for SAC
    value_lr: 3.0e-4  # Q-network learning rate
    adam_beta1: 0.9
    adam_beta2: 0.999
    adam_eps: 1.0e-08
    clip_grad: 10.0

  # Override the default values in training_backend/fsdp
  fsdp_config:
    strategy: "fsdp"
    sharding_strategy: "no_shard"
    mixed_precision:
      param_dtype: ${actor.model.precision}
      reduce_dtype: ${actor.model.precision}
      buffer_dtype: ${actor.model.precision}

reward:
  use_reward_model: False

critic:
  use_critic_model: False
